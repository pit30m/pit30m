{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from uuid import UUID\n",
    "import lz4\n",
    "from typing import List, Tuple, Dict, Union, Optional, Iterable\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pit30m.data.partitions import PreProcessPartition, GeoPartition, QueryBasePartition\n",
    "from pit30m.camera import CamName\n",
    "from pit30m import LogReader\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00682fa6-2183-4a0d-dcfe-bc38c448090f', '006ce26a-cd92-4b90-ec10-454da73ede13', '009334fa-acb9-4813-c60c-1916c83974c8', '00a05077-a8f6-4f08-ee38-c7bc4e885611', '0106abc2-b689-464a-cfe4-9f6a00089733', '017be362-c6cf-47fe-e317-fdcf128ce85a', '01959149-0840-4eba-e0a0-53c6b2bb0f84', '01c3ae4e-2bb6-4bd1-efc9-bfb16e4bd1e3', '01d09d4c-329a-4952-f2ed-555aa76dab2c', '01e21524-2e61-4ea6-cf98-164bcb2d29ab', '01e86404-c224-4b10-d12e-9e2144626b3b', '020465aa-b3f0-4b8e-c6af-b535078fe30c', '020829cb-e2a3-4160-df19-8bd4682f1115', '0209f084-2efb-4acf-f2ce-e8f8a58c8b06', '021286dc-5fe5-445f-e5fa-f875f2eb3c57', '024e8e83-7778-496f-e3c4-dfd7c0875d2d', '025c3909-7fe4-4817-e386-5d2de975363a', '025e793e-e8eb-4c6d-e074-accb77dbe4d3', '027a633e-c9d0-4340-fa28-50bf842e6d2c', '0281c09f-836a-4cfa-dcbd-77f1349b6b96', '0308a5d9-6e4c-498f-e7ef-c75e8ae721af', '0330dec6-1dbe-44d8-e139-723f2b7be005', '03359966-92d2-44d5-df29-3f391645f9ab', '03709ebf-bcce-43c5-dd63-9170b2a74836', '037e11f7-2300-47ad-e41d-9d87c2ec218f', '03b212bd-5d66-4924-dc39-3a4a72683f77', '03b711e5-bca3-429c-ce73-01784bd0e66c', '03d80bb8-0e4f-4122-dd66-35c40fb10ed7', '03e24906-57b2-465d-e203-9b1eb9d22f4e', '03e591f9-7558-4a2d-ccaa-ca44b61a2606', '040ad342-4909-4e7a-f38c-698a27763f8a', '047b37d0-e575-4d15-ff99-cbf646573ccd', '047fc632-d080-45fd-c009-4cd13edaceb5', '04d91c42-3077-4e92-ef50-02c41bba967a', '0525f22f-b9dd-43a5-d66a-a201015a363d', '05499467-1fb2-40ff-c0ff-4ca1fc52ea5a', '0568c89c-6562-4aa8-c82d-d43aa72dca45', '058a745e-697c-465c-ce55-789ce9e453a7', '05d90521-f44c-4983-de64-29e90cbb5d22', '061421eb-5240-4e60-c2a6-e9ea83a16625', '0617a7c4-57bb-4785-c06e-79fafc5716a4', '061e6a5d-ebc4-448b-d582-b81fb635ea70', '0667b559-a356-433b-c48b-7e99c5c068ad', '06a65334-eb5a-4141-d45a-bb6dcbb750a8', '06a9aa89-9477-4c50-c112-aa0fdce0e6bd', '06ca16c3-3461-40d9-ef1a-a743dd0d7731', '06d3cac5-c7a4-4843-e8c1-3f552d25b481', '07160994-465c-488e-c835-263b472eed59', '07fc0f31-031e-4ccd-cdd6-05102a0315b9', '07ff1ba3-da2a-46ba-e1d4-75d335c729bf']\n"
     ]
    }
   ],
   "source": [
    "with open(\"../pit30m/all_logs.txt\", \"r\") as f:\n",
    "    logs = f.readlines()\n",
    "    logs = sorted([x.strip() for x in logs])\n",
    "\n",
    "# logs = []\n",
    "# for f in glob.glob(\"/home/julieta/pit30m_partitions/utm_poses_dense/to_hide/*\"):\n",
    "# print(logs)\n",
    "#     logs.append(f.split(\"/\")[-1][:-4])\n",
    "\n",
    "logs = logs[:50]\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_get_sensor_index(log_reader: LogReader, sensor: str) -> Tuple[str, Optional[np.ndarray]]:\n",
    "    \"\"\"Try to fetch index from AWS, return None if it doesn't exist\"\"\"    \n",
    "    try:\n",
    "        if sensor == \"LIDAR\":\n",
    "            index = log_reader.get_lidar_geo_index()\n",
    "        elif sensor in CamName.__members__:\n",
    "            index = log_reader.get_cam_geo_index(CamName[sensor.upper()], max_delta_s=0.2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sensor {sensor}\")\n",
    "    except FileNotFoundError:\n",
    "        index = None\n",
    "    \n",
    "    return log_reader.log_id, index\n",
    "    \n",
    "\n",
    "def get_sensor_indices(\n",
    "    log_readers: Iterable[LogReader], sensor: str,\n",
    ") -> Tuple[Dict[UUID, np.ndarray], Dict[UUID, LogReader], Parallel]:\n",
    "    \"\"\"Get the indices that map poses to sensor observations\"\"\"\n",
    "    pool = Parallel(n_jobs=mp.cpu_count(), verbose=1, batch_size=8)\n",
    "    res = pool(delayed(maybe_get_sensor_index)(lr, sensor) for lr in log_readers)\n",
    "    sensor_indices = {log_id: index for (log_id, index) in res}\n",
    "    # log_readers = {log_id: lr for (log_id, lr) in zip(logs, log_readers)}\n",
    "    return sensor_indices, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_readers = {}\n",
    "for logid in logs:\n",
    "    # log_readers[logid] = LogReader(f\"s3://pit30m/{logid}/\", index_version=1)\n",
    "    log_readers[logid] = LogReader(\n",
    "        f\"s3://pit30m/{logid}/\", \n",
    "        index_version=2, \n",
    "        partitions=[PreProcessPartition.VALID, GeoPartition.TRAIN],\n",
    "        # partitions=[PreProcessPartition.VALID, GeoPartition.TEST, QueryBasePartition.QUERY],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 2.564s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 119.623s\n",
      "WARNING: There are 285 / 12197 (2.34%) timestamp associations with gap > 0.200s. Max is 12.821s\n",
      "WARNING: There are 3582 / 14142 (25.33%) timestamp associations with gap > 0.200s. Max is 142.284s\n",
      "WARNING: There are 245 / 12024 (2.04%) timestamp associations with gap > 0.200s. Max is 11.163s\n",
      "WARNING: There are 620 / 16626 (3.73%) timestamp associations with gap > 0.200s. Max is 29.139s\n",
      "WARNING: There are 1641 / 25374 (6.47%) timestamp associations with gap > 0.200s. Max is 79.552s\n",
      "WARNING: There are 3296 / 25108 (13.13%) timestamp associations with gap > 0.200s. Max is 39.960s\n",
      "WARNING: There are 58 / 23041 (0.25%) timestamp associations with gap > 0.200s. Max is 2.826s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 2.379s\n",
      "WARNING: There are 320 / 7070 (4.53%) timestamp associations with gap > 0.200s. Max is 14.953s\n",
      "WARNING: There are 6992 / 41334 (16.92%) timestamp associations with gap > 0.200s. Max is 589.232s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 6.358s\n",
      "WARNING: There are 945 / 14448 (6.54%) timestamp associations with gap > 0.200s. Max is 71.976s\n",
      "WARNING: There are 1891 / 22450 (8.42%) timestamp associations with gap > 0.200s. Max is 33.067s\n",
      "WARNING: There are 6838 / 33800 (20.23%) timestamp associations with gap > 0.200s. Max is 239.607s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 2.703s\n",
      "WARNING: There are 19549 / 30480 (64.14%) timestamp associations with gap > 0.200s. Max is 896.527s\n",
      "WARNING: There are 64 / 26815 (0.24%) timestamp associations with gap > 0.200s. Max is 5.913s\n",
      "WARNING: There are 5882 / 40719 (14.45%) timestamp associations with gap > 0.200s. Max is 168.017s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 2.676s\n",
      "WARNING: There are 633 / 29731 (2.13%) timestamp associations with gap > 0.200s. Max is 26.678s\n",
      "WARNING: There are 1 / 1 (100.00%) timestamp associations with gap > 0.200s. Max is 168.559s\n",
      "WARNING: There are 8164 / 37150 (21.98%) timestamp associations with gap > 0.200s. Max is 236.261s\n",
      "WARNING: There are 55 / 26181 (0.21%) timestamp associations with gap > 0.200s. Max is 5.358s\n",
      "WARNING: There are 35935 / 65724 (54.68%) timestamp associations with gap > 0.200s. Max is 3106.091s\n",
      "WARNING: There are 21258 / 66788 (31.83%) timestamp associations with gap > 0.200s. Max is 416.756s\n",
      "WARNING: There are 10811 / 76832 (14.07%) timestamp associations with gap > 0.200s. Max is 112.161s\n",
      "WARNING: There are 1998 / 64905 (3.08%) timestamp associations with gap > 0.200s. Max is 167.287s\n",
      "WARNING: There are 92 / 50181 (0.18%) timestamp associations with gap > 0.200s. Max is 3.427s\n",
      "WARNING: There are 311 / 17122 (1.82%) timestamp associations with gap > 0.200s. Max is 11.415s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figsz = 20 // 2\n",
    "fig = plt.figure(figsize=(figsz, figsz), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "included = 0\n",
    "with_poses = []\n",
    "missing = []\n",
    "\n",
    "# Load all the indices asynchronously\n",
    "sensor_indices, pool = get_sensor_indices(log_readers.values(), CamName.MIDDLE_FRONT_WIDE.name)\n",
    "\n",
    "for log_id, index in tqdm(sensor_indices.items()):\n",
    "\n",
    "    # if log_id == \"017be362-c6cf-47fe-e317-fdcf128ce85a\":\n",
    "    #     continue\n",
    "    # if log_id == \"01959149-0840-4eba-e0a0-53c6b2bb0f84\":\n",
    "    #     continue\n",
    "    \n",
    "    # camera index\n",
    "    if index is None:\n",
    "        print(f\"{log_id} has no index yet\")\n",
    "        missing.append(log_id)\n",
    "        continue\n",
    "    \n",
    "    utm_present = index[\"utm_present\"]\n",
    "    x = index[\"utm_x\"][utm_present]\n",
    "    y = index[\"utm_y\"][utm_present]\n",
    "    \n",
    "    # # UTM poses with offsets from the maps \n",
    "    # valid, xyzs = lr.utm_poses_dense\n",
    "    # xyzs = xyzs[valid]\n",
    "    # x, y = xyzs[:, 0], xyzs[:, 1]\n",
    "    # if len(x) > 0:\n",
    "    #     print(xyzs[0])\n",
    "    \n",
    "    # This gives me 3_249 poses\n",
    "    # pose_index = lr.continuous_pose_dense\n",
    "    # x, y = pose_index[:, 2], pose_index[:, 3]\n",
    "    \n",
    "    # MRP dense, removing invalid, I get 2_530 poses\n",
    "    # pose_index = lr.map_relative_poses_dense\n",
    "    # pose_index = pose_index[pose_index[\"valid\"]]\n",
    "    # x, y = pose_index[\"x\"], pose_index[\"y\"]\n",
    "     \n",
    "    sc = plt.scatter(x, y, s=0.5, alpha=0.5)\n",
    "    \n",
    "    # if len(x) > 0:\n",
    "    #     print(log_id, pose_index[0])\n",
    "    #     print(pose_index[\"time\"][0])\n",
    "    #     with_poses.append(log_id)\n",
    "    \n",
    "    included += len(x)\n",
    "    print(f\"{included:_} poses included so far\")\n",
    "    \n",
    "    # percent_hidden = 100 * hidden_in_log / len(utm)\n",
    "    # if percent_hidden > 90: \n",
    "    #     print(f\"{log_id} has {hidden_in_log:_} poses to hide ({percent_hidden:.2f}%)\")\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"Easting\")\n",
    "plt.ylabel(\"Northing\")\n",
    "print(f\"Total included: {included:_}\")\n",
    "print(f\"Total with poses: {len(with_poses)}\")\n",
    "print(with_poses)\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xyzs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xyzs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xyzs' is not defined"
     ]
    }
   ],
   "source": [
    "xyzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m submap_ids \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m([UUID(\u001b[39mbytes\u001b[39m\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mljust(\u001b[39m16\u001b[39m, \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\"\u001b[39m)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(index[\u001b[39m\"\u001b[39m\u001b[39msubmap_id\u001b[39m\u001b[39m\"\u001b[39m])])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(submap_ids))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(submap_ids[:\u001b[39m10\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "submap_ids = sorted([UUID(bytes=x.ljust(16, b\"\\x00\")) for x in np.unique(index[\"submap_id\"])])\n",
    "print(len(submap_ids))\n",
    "print(submap_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from numpy.lib import recfunctions as rfn\n",
    "\n",
    "def get_utm_poses_with_metadata(lr):\n",
    "    pose_time, pose_valid, pose_xy = lr.utm_poses_dense\n",
    "    time = rfn.unstructured_to_structured(pose_time[:, np.newaxis], names=[\"utm_time\"])\n",
    "    # assert np.all(np.diff(time[\"utm_time\"]) >= 0), \"Pose times are not sorted\"\n",
    "    valid = rfn.unstructured_to_structured(pose_valid[:, np.newaxis], names=[\"utm_valid\"])\n",
    "    # assert len(valid) == len(pose_valid)\n",
    "    xys = rfn.unstructured_to_structured(pose_xy, names=[\"utm_x\", \"utm_y\"])\n",
    "    # assert len(xys) == len(pose_xy)\n",
    "    sensor_poses = rfn.merge_arrays([time, valid, xys], flatten=True)\n",
    "\n",
    "    return lr.log_id, sensor_poses\n",
    "\n",
    "def get_utm_dense_poses(\n",
    "    logs: Iterable[UUID],\n",
    "    logreader_path: Path = Path(\"/mnt/data/pit30m/pose-backup-2023-04-18/\"),\n",
    "    # logreader_path: Path = Path(\"/mnt/data/pit30m/pose-backup-2023-04-18/\"),\n",
    "    pool: Parallel = None,\n",
    "    pickled: bool = None,\n",
    "):\n",
    "    \"\"\"Multiple ways to get dense poses\n",
    "    Args:\n",
    "        logs: list of log ids whose dense poses we want to load\n",
    "        logreader_path: Path where to load log readers from\n",
    "        pool: pool of parallel workers to load the poses\n",
    "        pickled: whether to load from a pre-dumped pickled file on disk\n",
    "    \"\"\"\n",
    "\n",
    "    log_readers = [LogReader(str(logreader_path / f\"{log}\")) for log in logs]\n",
    "\n",
    "    if pickled:\n",
    "        # If loading from pickle, ignore everything and just load from disk\n",
    "        with open(\"/home/julieta/pickles/utm_dense_poses.pkl\", \"rb\") as handle:\n",
    "            sensor_poses = pickle.load(handle)\n",
    "\n",
    "        # The pickle has everything -- filter for the ones that were asked for\n",
    "        sensor_poses = {log_id: sensor_pose for (log_id, sensor_pose) in sensor_poses.items() if log_id in logs}\n",
    "    else:\n",
    "        if pool:\n",
    "            res = pool(delayed(get_utm_poses_with_metadata)(lr) for lr in log_readers)\n",
    "            sensor_poses = {log_id: index for (log_id, index) in res}\n",
    "        else:\n",
    "            sensor_poses = {\n",
    "                log_id: get_utm_poses_with_metadata(lr)[1]\n",
    "                for (log_id, lr) in tqdm(\n",
    "                    zip(logs, log_readers), total=len(logs), desc=\"Getting dense poses sequentially from disk\"\n",
    "                )\n",
    "            }\n",
    "\n",
    "    log_readers = {log_id: lr for (log_id, lr) in zip(logs, log_readers)}\n",
    "    return sensor_poses, log_readers, pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_fpath = \"/mnt/data/pit30m/pose-backup-2023-04-18/556af30b-cb04-4a08-ee47-4eb9e79da376/all_poses.npz.lz4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pose_fpath, \"rb\") as in_compressed_f:\n",
    "    with lz4.frame.open(in_compressed_f, \"rb\") as wgs84_f:\n",
    "        poses = np.load(wgs84_f)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "[UUID('00a53038-3072-4115-f458-02769036233f'), UUID('02c07a67-6925-4d5e-c4e4-3a65b1e37b94'), UUID('07e78e05-8aba-421f-e1e4-27679cec69a7'), UUID('082c2a5a-7516-4f9e-fce0-6ab6eb3e0965'), UUID('0c40dc7d-1d68-4482-d48a-d3c71ff26ca4'), UUID('0cd730f5-08b5-4a91-e5c2-077134e3cb61'), UUID('0d864017-a690-4255-c225-551e3c48705f'), UUID('116570b4-6598-4c1a-da4f-948194f9d04b'), UUID('12e48274-21e0-4340-d3fa-101b21a1baab'), UUID('17b09d4c-2514-43c8-f3e3-8133a4ebc62e')]\n"
     ]
    }
   ],
   "source": [
    "submap_ids = sorted([UUID(bytes=x.ljust(16, b\"\\x00\")) for x in np.unique(poses[\"map_relative\"][\"submap\"])])\n",
    "print(len(submap_ids))\n",
    "print(submap_ids[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pit30m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
